Particle Filter README for Final project

    For the final project, we used an updated version of the particle filter developed in projects 2 and 3 for robot localization. The basic idea was the same, that we would have a randomly-generated pose cloud, and then for each pose, we would calculate the probability of that pose, given a set of laser readings. However, due to the specifics of the assignment, a few changes and optimizations could be made to the filter in order to attempt to increase accuracy and speed.
    First, we no longer needed the prediction step, since we are now working in a static, single-pass environment. In the input file, we are given a fairly accurate position, so we just generated a couple thousand poses within 1m and 5 degrees of that position. As suggested in class, we generated poses with a uniform distribution in this range.
    A second change we made to the algorithm was in our probability calculation. In general, the idea is to develop a probability model based on the expected error characteristics of the laser. More specifically, our original probability calculation used a 0.8 weighting on a uniform probability distribution, and only a 0.2 weighting on the calculated gaussian probability to the nearest object, using the distance transform. This helped keep the particle cloud from over-localizing during motion, since otherwise the cloud could put far too much probability on what it thinks is the best position, and lose the diversity that allows the robot to make corrections and possibly recover from a kidnapped-robot situation. However, this is not a major factor when only considering a localized, unmoving scenario. Therefore, we changed the weights to take into account the gaussian obstacle distance term more heavily, in order to try to find the absolute best positioning. Additionally, we added a cutoff distance, beyond which we assumed that laser readings corresponded to an unmapped obstacle. Specifically, if the laser reading registered more than a certain threshold away from a mapped object and the reading also was located inside the map boundaries, we didn't count that reading in the probability calculation at all. This helped to prevent the obstacles within the robot's field of view from skewing the localization.
    Finally, we also used the particle filter to differentiate between mapped objects and unmapped obstacles. Similar to the method above, we used the distance transform to make this distinction. Once the best pose is found, we looked at the lasers coming from that pose. If the reading was more than the threshold distance from any mapped object, we assumed it was an unmapped obstacle. Otherwise, we treated it as part of the map. For consistency between our probability model and our final output, we used the same distance threshold for both parts. A good value for this threshold seems to be 0.5m, although testing shows some advantages to higher values as well. The advantage of increasing this threshold seems to be that for higher values (up to 2m or so), we can localize better, since more points will be considered when calculating the probability. With an 0.5m cutoff, some laser readings at the periphary of the robot's view get skewed enough that even though they should be part of the wall, the calculation assumes them not to be, which will lead to worse localization. However, the disadvantage of a higher value is that we will assume objects close to the wall to be part of the wall, which could lead to more difficulty in distinguishing mapped features from unmapped obstacles.
